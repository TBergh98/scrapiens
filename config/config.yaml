# Scrapiens Configuration File

# File Paths
paths:
  # Base directory (can be overridden by BASE_DIR environment variable)
  base_dir: "//nas1/SCS4/UO_Biostatistica/Simonetto/Scraping"
  # Input directory for YAML configuration files
  input_dir: "input"
  # Output directory for extracted links
  output_dir: "all_links"
  # Unified links file (after deduplication)
  unified_links_file: "link_unificati.json"

# Input Files Configuration (YAML format)
input_files:
  # Sites configuration file (list of websites to scrape with their keywords)
  sites_file: "sites.yaml"
  # Keywords configuration file (email addresses and their keywords of interest)
  keywords_file: "keywords.yaml"

# Selenium Configuration
selenium:
  # Run browser in headless mode
  headless: true
  # Browser to use (currently only 'chrome' supported)
  browser: "chrome"
  # Maximum time to wait for elements (seconds)
  implicit_wait: 15
  # Maximum time to wait for page load (seconds)
  page_load_timeout: 30
  # Save screenshots on pagination errors for debugging
  screenshot_on_error: true
  # Initial wait time after page load (seconds)
  initial_wait: 2
  # Wait time after cookie acceptance (seconds)
  cookie_wait: 1

# Web Scraping Configuration
scraping:
  # Default maximum pages to scrape per site
  max_pages_default: 1
  # Number of scroll iterations for JavaScript sites
  scroll_iterations: 50
  # Number of retry attempts for pagination clicks
  pagination_retries: 3
  # Enable JavaScript-based scrolling for dynamic content
  enable_js_scrolling: true
  # Maximum time to wait for page change after pagination (seconds)
  page_change_timeout: 8
  # Extended wait time for content loading (seconds)
  extended_wait: 5
  # Time between scroll iterations (seconds)
  scroll_delay: 1.5

# Cookie Banner Configuration
cookies:
  # Text patterns to search for in cookie buttons (case insensitive)
  text_patterns:
    - "accept"
    - "accetta"
    - "accept all"
    - "accetta tutti"
    - "accept cookies"
    - "accetta cookie"
    - "i agree"
    - "agree"
    - "acconsento"
    - "ok"
    - "continue"
    - "continua"
    - "allow"
    - "consenti"
    - "consent"
  # CSS selectors for cookie buttons
  attribute_selectors:
    - 'button[id*="accept"]'
    - 'button[class*="accept"]'
    - 'button[class*="cookie"]'
    - 'button[class*="consent"]'
    - 'a[id*="accept"]'
    - 'a[class*="accept"]'
    - 'a[class*="cookie"]'
    - '[data-testid*="accept"]'
    - '[data-testid*="cookie"]'
    - '[aria-label*="accept"]'
    - '[aria-label*="accetta"]'
    - '[aria-label*="cookie"]'

# Overlay Hiding Configuration
overlays:
  # CSS selectors for overlays to hide
  selectors:
    - ".eui-language-selector-button__language-code"
    - ".eui-toolbar"
    - ".eui-toolbar__left"
    - ".eui-toolbar__right"
    - ".eui-input-group-addon"
    - '[role="dialog"]'
    - ".modal"
    - ".overlay"
    - ".cookie-banner"
    - ".notification"

# OpenAI Configuration
openai:
  # Model to use for link classification
  model: "gpt-4o-mini"
  # Request timeout (seconds)
  timeout: 300
  # Progress indicator interval (seconds)
  progress_interval: 10
  # Classification prompt template
  classification_prompt: |
    Analyze the following list of URLs and classify each one into one of these categories:
    
    1. "single_grant" - URLs that lead to a page containing complete information about a SINGLE research grant/call (look for keywords like "bando", "call", "grant", "funding opportunity")
    2. "grant_list" - URLs that lead to a page containing a LIST of MULTIPLE grants/calls
    3. "other" - URLs that are generic pages (contacts, about us, home page, etc.)
    
    Return your response as a JSON array where each object has:
    - "url": the original URL
    - "category": one of "single_grant", "grant_list", or "other"
    - "confidence": a number between 0 and 1 indicating your confidence
    - "reason": a brief explanation of why you classified it this way
    
    URLs to classify:
    {urls}

# Grant Details Extraction Configuration
extractor:
  # Model to use for grant details extraction
  model: "gpt-4o"
  # Selenium page load timeout (seconds)
  timeout: 10
  # Number of parallel workers for extraction
  parallel_workers: 10
  # Maximum retry attempts for API failures
  max_retries: 3
  # Extraction prompt template
  extraction_prompt: |
    Analyze the following HTML content from a research grant/call page and extract the following information:
    
    1. **title**: The title of the grant/call (string)
    2. **organization**: The organization offering the grant (string)
    3. **abstract**: A brief summary or description of the grant (string, max 500 characters)
    4. **deadline**: The application deadline (CRITICAL: ONLY if EXPLICITLY stated in the text. Return null if not found or ambiguous. Format as YYYY-MM-DD. DO NOT invent or guess dates.)
    5. **funding_amount**: The funding amount available (string, e.g., "â‚¬500,000" or "up to $1M")
    
    IMPORTANT RULES:
    - Return ONLY factual information that is explicitly present in the HTML
    - For deadline: If you see text like "scadenza: 31/12/2024" or "deadline: December 31, 2024", extract it. If dates are vague like "coming soon" or "TBD", return null
    - For funding_amount: Look for keywords like "importo", "finanziamento", "budget", "funding", "amount"
    - If any field cannot be found, return null (not empty string)
    - Return valid JSON only
    
    URL: {url}
    
    HTML Content (truncated to first 8000 chars):
    {html}
    
    Return a JSON object with these exact keys: title, organization, abstract, deadline, funding_amount

# Cache Configuration
cache:
  # Enable caching of extracted grant details
  enabled: true
  # Cache file name (will be saved in output_dir)
  file: "grants_cache.json"

# Logging Configuration
logging:
  # Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  level: "INFO"
  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  # Log to file
  log_to_file: true
  # Log file name
  log_file: "scrapiens.log"
